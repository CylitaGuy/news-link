{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News-Link: Data Cleaning\n",
    "\n",
    "To build the corpus of news articles associated with the **News-Link** app, I have been scraping articles, daily from the CBC.ca. I will now merge and clean all of these files. I will also isolate links inserted by journalists to prepare a separate csv file that has links to additional stories that require scraping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python necessary packages and functions \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "#specifying local system path for remaining functions files\n",
    "sys.path.insert(1, '/Users/Cylita/Desktop/insight-ds-project_news-link/scripts')\n",
    "\n",
    "#self written functions and scripts that will be needed\n",
    "import text_normalization_funs as TN\n",
    "import similiarity_funcs as SIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in required raw data for processing, dropping all rows that contain NAs\n",
    "newsJan20 = pd.read_csv('/Users/Cylita/Desktop/insight-ds-project_news-link/data/raw/Jan_20_news.csv').dropna(subset=['title', 'maintext'])\n",
    "newsJan21 = pd.read_csv('/Users/Cylita/Desktop/insight-ds-project_news-link/data/raw/Jan_21_news.csv').dropna(subset=['title', 'maintext'])\n",
    "newsJan23 = pd.read_csv('/Users/Cylita/Desktop/insight-ds-project_news-link/data/raw/Jan_23_news.csv').dropna(subset=['title', 'maintext'])\n",
    "newsJan24 = pd.read_csv('/Users/Cylita/Desktop/insight-ds-project_news-link/data/raw/Jan_24_news.csv').dropna(subset=['title', 'maintext'])\n",
    "newsJan26 = pd.read_csv('/Users/Cylita/Desktop/insight-ds-project_news-link/data/raw/Jan_26_news.csv').dropna(subset=['title', 'maintext'])\n",
    "newsJan27 = pd.read_csv('/Users/Cylita/Desktop/insight-ds-project_news-link/data/raw/Jan_27_news.csv').dropna(subset=['title', 'maintext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining all the dataframes\n",
    "combine_news = pd.concat([newsJan20, newsJan21, newsJan23, newsJan24, newsJan26, newsJan27], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting similar links inserted by journalists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1221"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Likely some duplicate stories referenced over different days and different landing pages\n",
    "#Removing duplications\n",
    "unique_news = combine_news.drop_duplicates(subset = 'mainurl')\n",
    "print(len(unique_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Isolating individual similar and realted links by storying them in separate columns (right now all in the same)\n",
    "splitlinks = unique_news['simlinks'].str.split(' ', expand=True)\n",
    "#Some articles have a lot of links. To take a manageable sample I will reduce this to only take the first 4 similar \n",
    "#links inserted by a journalist\n",
    "splitlinks_reduced = splitlinks.loc[:,1:4]\n",
    "#Renaming columns\n",
    "splitlinks_foradding = splitlinks_reduced.rename(columns={1:\"sim1\", 2:'sim2', 3:'sim3', 4:'sim4'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8c5852bb250d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Adding the first four similar links as four new columns to the exisiting dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mUniqLinkNews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munique_news\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplitlinks_foradding\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Export as a newdataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mUniqLinkNews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cleaned_Split_RawNews.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#Adding the first four similar links as four new columns to the exisiting dataframe\n",
    "UniqLinkNews = pd.concat([unique_news, splitlinks_foradding], axis=1)\n",
    "\n",
    "#Exporting as a dataframe\n",
    "#This dataframe will be used to scrape similar link articles using webscraping functions \n",
    "#(see scraping similar articles script)\n",
    "UniqLinkNews.to_csv('Cleaned_Split_RawNews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combing all news stories to create corpus\n",
    "\n",
    "I have already used the above dataframe to scrape simialar stories inserted by journalists. I will now be adding these stories to the original news stories that I scraped. The resulting dataframe will be used to help validate the approach I have chosen (see validation scripts). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in data frame with scraped web articles\n",
    "sim_news = pd.read_csv(\"/Users/Cylita/Desktop/insight-ds-project_news-link/data/processed/News_Raw_ValData.csv\").dropna(subset=['title', 'maintext'])\n",
    "#Triming to only relevent columns\n",
    "trimmed_sim = sim_news[['author', 'date','title','maintext', 'mainurl']]\n",
    "\n",
    "#Reading in the original dataframe where all similar links were split into columns (i.e. partial scraped news corpus)\n",
    "UniqLinkNews = pd.read_csv('/Users/Cylita/Desktop/insight-ds-project_news-link/data/processed/Cleaned_Split_RawNews.csv')\n",
    "trimmed_full = UniqLinkNews[['author','date', 'title', 'maintext', 'mainurl', 'simlinks', 'sim1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining the above two dataframes\n",
    "raw_val = pd.concat([trimmed_full, trimmed_sim], axis=0, ignore_index = True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the above combined dataframe - creating combined text (story + title) and cleaning in preparation for NLP \n",
    "#Combing the news story headlines with the maintext for the stories\n",
    "full_text = raw_val[\"title\"].map(str)+ '. ' + raw_val[\"maintext\"]\n",
    "#Preprocessing all text to conduct any NLP analysis\n",
    "normfull = TN.normalize_NewsText(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-78d364c05ccd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Adding in cleaned text to dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mraw_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mraw_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cleaned_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormfull\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'full_text' is not defined"
     ]
    }
   ],
   "source": [
    "#Adding in cleaned text to dataframe\n",
    "raw_val['full_text'] = full_text\n",
    "raw_val['cleaned_text'] = normfull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting cleaned and processed text to a corpus document\n",
    "raw_val.to_csv(\"Val_Corpus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading back in the new dataframe to clean up, based on the external validation script run\n",
    "val_corpus = pd.read_csv(\"Val_Corpus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing all duplicates from the full news corpus that will be used in the project\n",
    "unique_val = val_corpus.drop_duplicates(subset=['mainurl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Isolating the first 30% of a news story, based on cleaned full text\n",
    "reduced_text = []\n",
    "\n",
    "for doc in unique_val['cleaned_text']:\n",
    "    redu = TN.Clean30(doc)\n",
    "    reduced_text.append(redu)\n",
    "\n",
    "unique_val[\"reduced\"] = reduced_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting cleaned and processed text to a corpus document\n",
    "unique_val.to_csv(\"Final_Cleaned_Corpus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
